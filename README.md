# MathContentSimilarity

This repository is dedicated to the fine-tuning of the LLaMA2 model on a gold standard dataset to enhance its performance in recommending content from mathematical research papers. LLaMA2 is a cutting-edge language model designed for various natural language understanding tasks, and this project aims to leverage its capabilities for the specific task of recommending content within mathematical literature.

## About LLaMA2

LLaMA2 is a state-of-the-art language model that has proven its efficacy in a wide range of natural language understanding tasks. By building upon its pre-trained capabilities and adapting it to the specific requirements of math paper content recommendation, we can create a powerful tool for researchers, students, and professionals in the mathematical field.

## Getting Started

To get started with this project, follow these steps:

1. Clone this repository to your local machine:

   ```bash
   git clone https://github.com/your-username/math-papers-recommendation.git


### Related articles/papers:

1. [Parameter Efficient Fine Tuning (PEFT)](https://ojs.aaai.org/index.php/AAAI/article/view/26505/26277)
2. LORA: Low Rank Adaptation of Large Language Models(https://arxiv.org/pdf/2106.09685.pdf)
3. [Dataset on huggingface](https://huggingface.co/docs/datasets/create_dataset)
4. [Article on Example fine tuning using PEFT]https://towardsdatascience.com/fine-tuning-large-language-models-llms-23473d763b91
